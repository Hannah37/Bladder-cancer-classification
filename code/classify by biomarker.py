# -*- coding: utf-8 -*-
"""biomarker

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1adS4qu97pF0ow2OkfqEslaBRhZmsWwim

## **Load data**
"""

from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %pwd
# %cd 'gdrive/My Drive/bladder cancer'
# %ls

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Commented out IPython magic to ensure Python compatibility.
# xlsx = pd.read_excel('./IMvigor210.xlsx', sheet_name = 'TMM norm' )
# xlsx.to_csv('./tmm.csv',index=False)
# print(xlsx.columns)
# %ls

data = pd.read_csv("IMvigor210_clin.csv")
print(data.columns)
print(len(data))

marker = pd.read_csv("tmm.csv")
ori_marker = marker

marker.columns

"""## **Data Preprocessing**"""

marker = marker.rename(columns = {'Symbol':'SampleID'}) 
marker = marker.drop(['Entrez Gene ID'], axis = 1).T
marker.columns = marker.iloc[0]
marker = marker.drop(marker.index[0])

print(marker.head())

len(marker), len(marker.columns)

target_column = 'Baseline ECOG Score'

## match non-digit labels to digits

# data['Best Confirmed Overall Response'] = data['Best Confirmed Overall Response'].map({'CR':0, 'PR':1, 'SD':2, 'PD':3, 'NE':4})
# data['Enrollment IC'] = data['Enrollment IC'].map({'IC0':0, 'IC1':1, 'IC2':2})

# NA = -1 -> nan 빼고 해보기
# data['Immune phenotype'] = data['Immune phenotype'].map({'desert':0, 'inflamed':1, 'excluded':2})

# data['Sex'] = data['Sex'].map({'M':0, 'F':1})
# # NA = -1, rm 'NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER' since the number of it is 1
# data['Race'] = data['Race'].map({'BLACK OF AFRICAN AMERICAN':0, 'ASIAN':1, 'WHITE':2,  'OTHER':4})
# data['Race'] = data['Race'].fillna(-1)
# data['Intravesical BCG administered'] = data['Intravesical BCG administered'].map({'N':0, 'Y':1})
# data['Tobacco Use History'] = data['Tobacco Use History'].map({'NEVER':0, 'PREVIOUS':1, 'CURRENT':2})

# # # replace NA to the mean of N, Y
# # data['Met Disease Status'] = data['Met Disease Status'].map({'Liver':0, 'Visceral':1, 'LN Only':2, 'NA': -1})
# data['Sample age'] = data['Sample age'].map({'(less than) 1 year':0, '1-2 years':1, 'more than 2 years':2})
# data['Received platinum'] = data['Received platinum'].map({'N':0, 'Y':1})

# # replace NA to the mean of N, Y
# #data['Sample collected pre-platinum'] = data['Sample collected pre-platinum'].map({'N':0, 'Y':1, 'NA': 0.5})
# data['Lund'] = data['Lund'].map({'MS1a':0, 'MS1b':1, 'MS2a1':2,'MS2a2':3, 'MS2b1':4, 'MS2b2.1':5, 'MS2b2.2':6})
# data['Lund2'] = data['Lund2'].map({'UroA':0, 'UroB':1, 'Genomically unstable':3, 'Infiltrated':4, 'Basal/SCC-like':5})
# data['TCGA Subtype'] = data['TCGA Subtype'].map({'I':0, 'II':1, 'III':2, 'IV':3})     

# # replace NA to the mean of column 
# data['Neoantigen burden per MB'] = data['Neoantigen burden per MB'].fillna(data.mean()['Neoantigen burden per MB'])
# # replace NA to the mean of column 
# data['FMOne mutation burden per MB'] = data['FMOne mutation burden per MB'].fillna(data.mean()['FMOne mutation burden per MB'])

  

print(data[target_column])

if target_column in marker.columns:
  marker = marker.drop(target_column, axis = 1)

data.index = data['SampleID']
marker.index.equals(data.index)

marker = marker.join(data[target_column])
marker = marker.reset_index()
marker = marker.rename(columns = {'index':'SampleID'}) 
marker.head()

marker.index

marker = marker.dropna(axis=0)
marker.shape

marker.reset_index(inplace=True, drop=True)
marker.index

######17694 check
marker.shape

## split train and test data

from sklearn.model_selection import StratifiedShuffleSplit
split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=1004)

for train_idx, test_idx in split.split(marker, marker[target_column]):
    train = marker.loc[train_idx]
    test = marker.loc[test_idx]

print(len(train), len(test))

print(train.groupby(target_column).size())
print(test.groupby(target_column).size())

###check

## encode y to one-hot vector

from sklearn.preprocessing import LabelEncoder
encoder =  LabelEncoder()

decoded_y_train = train[target_column]
Y = encoder.fit_transform(decoded_y_train)
y_train = pd.get_dummies(Y).values

decoded_y_test = test[target_column]
Y = encoder.fit_transform(decoded_y_test)
y_test = pd.get_dummies(Y).values

x_train = train.drop([target_column], axis = 1)
x_test = test.drop([target_column], axis = 1)

x_train.shape, x_test.shape, y_train.shape, y_test.shape

# number of class
y_train.shape[1]

features = marker.columns.tolist()
features.remove(target_column)
features.remove('SampleID')
len(features)

x_train.set_index(x_train['SampleID'], inplace=True, drop=True)
x_test.set_index(x_test['SampleID'], inplace=True, drop=True)

x_train.drop(['SampleID'], axis=1, inplace=True)
x_test.drop(['SampleID'], axis=1, inplace=True)
x_train.reset_index(drop=True, inplace=True)
x_test.reset_index(drop=True, inplace=True)

x_train

x_test = np.asarray(x_test).astype(np.float32)
x_train = np.asarray(x_train).astype(np.float32)

"""# **Neural Networks**"""

from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import Adam
from keras import regularizers


model = Sequential()

model.add(Dense(64, input_shape=(len(features),), activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(64, activation='relu', kernel_regularizer='l1'))
model.add(Dense(y_train.shape[1], activation='softmax'))

opt = Adam(learning_rate=0.00001)
model.compile(loss='categorical_crossentropy', 
              optimizer=opt, 
              metrics=['accuracy']) 


model.summary()

hist = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=800)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

plt.figure(figsize=(12,8))
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.plot(hist.history['accuracy'])
# plt.plot(hist.history['recall'])
# plt.plot(hist.history['precision'])
plt.plot(hist.history['val_accuracy'])
plt.legend(['loss','val_loss', 'acc','val_acc'])
#plt.legend(['loss','val_loss', 'acc','val_acc', 'recall', 'precision'])
plt.grid()
plt.show()

loss, accuracy = model.evaluate(x_test, y_test)
print("Accuracy = {:.3f}".format(accuracy))

prediction = model.predict(x_test, verbose=1)
prediction.shape

one_pred = np.zeros_like(prediction)
one_pred[np.arange(len(prediction)), prediction.argmax(1)] = 1
# one_pred

# one_pred

marker.groupby(target_column).size()

one_pred.shape

from keras import backend as K
import tensorflow as tf

def result(one_pred):
  for target_class in range(y_train.shape[1]):
    """RECALL"""

    tp = K.sum(one_pred[:, target_class] * y_test[:, target_class])  #dtype=float32
    tpfn = tf.cast(K.sum(y_test[:, target_class]), tf.float32)  #uint8 to float32

    # Recall =  (True Positive) / (True Positive + False Negative)
    recall = tp / (tpfn + K.epsilon())

    """PRECISION"""

    tpfp = tf.cast(K.sum(one_pred[:, target_class]), tf.float32)

    # Precision = (True Positive) / (True Positive + False Positive)
    precision = tp / (tpfp + K.epsilon())

    """Accuracy"""
    tn = np.sum((one_pred[:, target_class]==0) * (y_test[:, target_class]==0).T)

    accuracy = (tp + tn)/(tpfn + tpfp - tp + tn)

    print("\nclass: ", target_class)
    print("tp: ", tp)
    print("tp+fn: ", tpfn)
    print("tp+fp: ", tpfp)

    tf.print("Accuracy = {:.3f}".format(accuracy))
    tf.print("Recall = {:.3f}".format(recall))
    tf.print("Precision = {:.3f} ".format(precision))

result(one_pred)



"""# **SVM**"""

# Support Vector Classifier

from sklearn.svm import SVC

# C=slack variable weight
# 데이터가 선형분리가 불가능한 경우, 슬랙변수로 개별적인 오차를 허용한다. 
svm = SVC(kernel='linear', C=10, random_state=0)

svm.fit(x_train, decoded_y_train)
y_pred_svc = svm.predict(x_test)

print(y_pred_svc)

# encode y_pred_svc to one-hot vector 
Y = encoder.fit_transform(y_pred_svc)
y_pred_svc = pd.get_dummies(Y).values.astype(np.float32)

# number of supports
svm.n_support_

x_train.shape

y_pred_svc.shape

# output개수가 0인 class가 있을 때 add [0]*y_pred_svc.shape[0]

new_column = [0] * y_pred_svc.shape[0]
y_pred_svc = np.insert(y_pred_svc, 2, new_column, axis=1)
# y_pred_svc

result(y_pred_svc)



"""#**PCA**#"""

from sklearn.preprocessing import StandardScaler

# drop_col = [target_column, 'SampleID'] 
drop_col = ['Symbol', 'Entrez Gene ID']

marker = ori_marker
x = marker.drop(drop_col, axis=1).values
# print(x)
y = data.loc[:,[target_column]].values
# y = data[target_column].values

x = StandardScaler().fit_transform(x)
# x

from sklearn.decomposition import PCA

pca = PCA(n_components=2)
PCs = pca.fit_transform(x)

# pc_col = (range(2))
df = pd.DataFrame(data= PCs, columns= ['PC'+str(i) for i in range(1,3)])

df.head()

df.shape

y.shape

pca.explained_variance_ratio_

sum(pca.explained_variance_ratio_)

no_dup_df = data.drop_duplicates(subset=target_column)
target_val = no_dup_df[target_column].dropna(axis=0).tolist()
# target_val = no_dup_df[target_column].tolist()
target_val

# target_val.count(2)

data[target_column].reset_index(drop=True, inplace=True)

data[target_column]

pc_target_df = pd.concat([df, data[target_column]], axis=1)

pc_target_df

pc_target_df.shape

fig = plt.figure(figsize = (8, 8))
ax = fig.add_subplot(1, 1, 1)
ax.set_xlabel('PC1', fontsize = 15)
ax.set_ylabel('PC2', fontsize = 15)
ax.set_title('2 component PCA', fontsize=20)


colors = ['r', 'g', 'b']
for target, color in zip(target_val, colors):
    indicesToKeep = pc_target_df[target_column] == target
    ax.scatter(pc_target_df.loc[indicesToKeep, 'PC1']
               , pc_target_df.loc[indicesToKeep, 'PC2']
               , c = color
               , s = 50)
ax.legend(target_val)
ax.grid()

pca2 = PCA(n_components=3)
PCs2 = pca2.fit_transform(x)
df2= pd.DataFrame(data = PCs2
             , columns = ['PC'+str(i) for i in range(1,4)])

pc_target_df2 = pd.concat([df2, data[target_column]], axis=1)

from mpl_toolkits.mplot3d import Axes3D

fig2 = plt.figure(figsize=(15,15))
ax2 = fig2.add_subplot(111, projection='3d')

ax2.set_xlabel('PC1', fontsize = 15)
ax2.set_ylabel('PC2', fontsize = 15)
ax2.set_zlabel('PC3', fontsize = 15)
ax2.set_title('3 Component PCA', fontsize = 20)

colors = ["#7fc97f","#beaed4","#fdc086","#ffff99","#386cb0","#f0027f","","#666666"]
for target, color in zip(target_val, colors):
  indicesToKeep = pc_target_df2[target_column] == target
  ax2.scatter(pc_target_df2.loc[indicesToKeep, 'PC1']
               , pc_target_df2.loc[indicesToKeep, 'PC2']
               , pc_target_df2.loc[indicesToKeep, 'PC3']
               , c = color
               , s = 30)

ax2.legend(target_val)
ax2.grid()

